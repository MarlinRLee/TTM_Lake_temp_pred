{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 7 Tiny time mixer code. This should work on correctly set up data but I do not have our data so I can not vouch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Adapt real code from https://github.com/ibm-granite/granite-tsfm/blob/main/notebooks/tutorial/ttm_channel_mix_finetuning.ipynb\n",
    "\n",
    "### TODO Get real data into Tidy CSV format (ie each row is a column)\n",
    "\n",
    "### Current code is from https://colab.research.google.com/github/ibm-granite/granite-tsfm/blob/main/notebooks/hfdemo/ttm_getting_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 24.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /users/9/lee02328/.conda/envs/csci8523\n",
      "\n",
      "  added / updated specs:\n",
      "    - lxml\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.8 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openssl                                  3.3.2-hb9d3cd8_0 --> 3.4.0-hb9d3cd8_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-3.4.0        | 2.8 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "! conda update -n base -c defaults conda -y\n",
    "! conda install lxml -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(537): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: / ^C\n",
      "failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    }
   ],
   "source": [
    "#! conda install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "#! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.14\" -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "\n",
    "from tsfm_public import TrackingCallback, count_parameters, load_dataset\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.visualization import plot_predictions\n",
    "\n",
    "from tsfm_public import (\n",
    "    TimeSeriesPreprocessor,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    "    get_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_625655/3024688260.py:4: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  data = pd.read_csv(url, on_bad_lines = \"warn\", delimiter=\",\", verbose = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 2.06 ms\n",
      "Type conversion took: 2.78 ms\n",
      "Parser memory cleanup took: 0.00 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#1995\n",
    "url = \"https://apps.glerl.noaa.gov/erddap/griddap/GLSEA_GCS.csvp?sst%5B(1995-01-01T12:00:00Z):1:(2023-12-31T12:00:00Z)%5D%5B(42.05651963):1:(42.05649963)%5D%5B(-87.66870036):1:(-87.66867036)%5D\"\n",
    "data = pd.read_csv(url, on_bad_lines = \"warn\", delimiter=\",\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  sst (degree_C)\n",
      "0 1995-01-01 12:00:00+00:00            4.49\n",
      "1 1995-01-02 12:00:00+00:00            4.43\n",
      "2 1995-01-03 12:00:00+00:00            4.35\n",
      "3 1995-01-04 12:00:00+00:00            4.26\n",
      "4 1995-01-05 12:00:00+00:00            4.17\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m filtered_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLSEA_SST_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;66;03m#date\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      7\u001b[0m _, _, dset_test \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m----> 8\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[43mdataset_name\u001b[49m,\n\u001b[1;32m      9\u001b[0m     context_length\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m     10\u001b[0m     forecast_length\u001b[38;5;241m=\u001b[39mforecast_length,\n\u001b[1;32m     11\u001b[0m     fewshot_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     12\u001b[0m     dataset_path\u001b[38;5;241m=\u001b[39mDATASET_PATH,\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_data = data.dropna()\n",
    "filtered_data['time (UTC)'] = pd.to_datetime(filtered_data['time (UTC)'])\n",
    "filtered_data = filtered_data.rename(columns={'time (UTC)': 'date'})\n",
    "filtered_data = filtered_data.drop([\"latitude (degrees_north)\", \"longitude (degrees_east)\"], axis=1)\n",
    "filtered_data.to_csv(\"GLSEA_SST_data.csv\", index=False)#date\n",
    "print(filtered_data.head())\n",
    "_, _, dset_test = load_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    context_length=512,\n",
    "    forecast_length=1,\n",
    "    fewshot_fraction=1.0,\n",
    "    dataset_path=\"GLSEA_SST_data.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 69\n",
    "set_seed(SEED)\n",
    "\n",
    "# TTM Model path. The default model path is Granite-R2. Below, you can choose other TTM releases.\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "# TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
    "# TTM_MODEL_PATH = \"ibm/ttm-research-r2\"\n",
    "\n",
    "# Context length, Or Length of the history.\n",
    "# Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "# Dataset\n",
    "# The dataloaders will utilize the easy-to-use YAML configurations defined below.\n",
    "# Dataset configuration YAMLS: https://github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/resources/data_config\n",
    "# Note that `dataset_root_path` can also be provided instead of `dataset_path` to the `load_dataset()` function to\n",
    "# run this notebook on already downloaded dataset.\n",
    "# Check the `load_dataset()` function to see more functionalities.\n",
    "TARGET_DATASET = \"etth1\"\n",
    "DATASET_PATH = \"GLSEA_SST_data.csv\"\n",
    "\n",
    "# Results dir\n",
    "OUT_DIR = \"ttm_finetuned_models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_eval(dataset_name, batch_size, context_length=512, forecast_length=96):\n",
    "    # Get data\n",
    "    _, _, dset_test = load_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        context_length=context_length,\n",
    "        forecast_length=forecast_length,\n",
    "        fewshot_fraction=1.0,\n",
    "        dataset_path=DATASET_PATH,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    zeroshot_model = get_model(TTM_MODEL_PATH, context_length=context_length, prediction_length=forecast_length)\n",
    "\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # zeroshot_trainer\n",
    "    zeroshot_trainer = Trainer(\n",
    "        model=zeroshot_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=temp_dir,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            seed=SEED,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "    )\n",
    "    # evaluate = zero-shot performance\n",
    "    print(\"+\" * 20, \"Test MSE zero-shot\", \"+\" * 20)\n",
    "    zeroshot_output = zeroshot_trainer.evaluate(dset_test)\n",
    "    print(zeroshot_output)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=zeroshot_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_zeroshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_finetune_eval(\n",
    "    dataset_name,\n",
    "    batch_size,\n",
    "    learning_rate=None,\n",
    "    context_length=512,\n",
    "    forecast_length=96,\n",
    "    fewshot_percent=5,\n",
    "    freeze_backbone=True,\n",
    "    num_epochs=50,\n",
    "    save_dir=OUT_DIR,\n",
    "):\n",
    "    out_dir = os.path.join(save_dir, dataset_name)\n",
    "\n",
    "    print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "    dset_train, dset_val, dset_test = load_dataset(\n",
    "        dataset_name,\n",
    "        context_length,\n",
    "        forecast_length,\n",
    "        fewshot_fraction=fewshot_percent / 100,\n",
    "        dataset_path=DATASET_PATH,\n",
    "    )\n",
    "\n",
    "    # change head dropout to 0.7 for ett datasets\n",
    "    if \"ett\" in dataset_name:\n",
    "        finetune_forecast_model = get_model(\n",
    "            TTM_MODEL_PATH, context_length=context_length, prediction_length=forecast_length, head_dropout=0.7\n",
    "        )\n",
    "    else:\n",
    "        finetune_forecast_model = get_model(\n",
    "            TTM_MODEL_PATH, context_length=context_length, prediction_length=forecast_length\n",
    "        )\n",
    "\n",
    "    if freeze_backbone:\n",
    "        print(\n",
    "            \"Number of params before freezing backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "        # Freeze the backbone of the model\n",
    "        for param in finetune_forecast_model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Count params\n",
    "        print(\n",
    "            \"Number of params after freezing the backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "    # Find optimal learning rate\n",
    "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
    "    if learning_rate is None:\n",
    "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
    "            finetune_forecast_model,\n",
    "            dset_train,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
    "\n",
    "    print(f\"Using learning rate = {learning_rate}\")\n",
    "    finetune_forecast_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"output\"),\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        dataloader_num_workers=8,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create the early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
    "    )\n",
    "    tracking_callback = TrackingCallback()\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
    "    )\n",
    "\n",
    "    finetune_forecast_trainer = Trainer(\n",
    "        model=finetune_forecast_model,\n",
    "        args=finetune_forecast_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val,\n",
    "        callbacks=[early_stopping_callback, tracking_callback],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
    "\n",
    "    # Fine tune\n",
    "    finetune_forecast_trainer.train()\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
    "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "    print(fewshot_output)\n",
    "    print(\"+\" * 60)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=finetune_forecast_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_fewshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:p-625655:t-139937847189824:data_handling.py:load_dataset:Dataset name: etth1, context length: 512, prediction length 96\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Selection would result in an empty time series, please check start_index and time series length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mzeroshot_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_DATASET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONTEXT_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m, in \u001b[0;36mzeroshot_eval\u001b[0;34m(dataset_name, batch_size, context_length, forecast_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeroshot_eval\u001b[39m(dataset_name, batch_size, context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, forecast_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Get data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     _, _, dset_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforecast_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforecast_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfewshot_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     zeroshot_model \u001b[38;5;241m=\u001b[39m get_model(TTM_MODEL_PATH, context_length\u001b[38;5;241m=\u001b[39mcontext_length, prediction_length\u001b[38;5;241m=\u001b[39mforecast_length)\n",
      "File \u001b[0;32m~/.conda/envs/csci8523/lib/python3.10/site-packages/tsfm_public/toolkit/data_handling.py:75\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset_name, context_length, forecast_length, fewshot_fraction, fewshot_location, dataset_root_path, dataset_path, use_frequency_token, enable_padding)\u001b[0m\n\u001b[1;32m     68\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m Path(dataset_root_path) \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     70\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m     71\u001b[0m     dataset_path,\n\u001b[1;32m     72\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39m[config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_column\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 75\u001b[0m train_dataset, valid_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtsp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfewshot_fraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfewshot_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_frequency_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_frequency_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_padding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData lengths: train = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, valid_dataset, test_dataset\n",
      "File \u001b[0;32m~/.conda/envs/csci8523/lib/python3.10/site-packages/tsfm_public/toolkit/time_series_preprocessor.py:843\u001b[0m, in \u001b[0;36mget_datasets\u001b[0;34m(ts_preprocessor, dataset, split_config, stride, fewshot_fraction, fewshot_location, as_univariate, use_frequency_token, enable_padding)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeSeriesPreprocessor must be instantiated with non-null prediction_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    841\u001b[0m data \u001b[38;5;241m=\u001b[39m ts_preprocessor\u001b[38;5;241m.\u001b[39m_standardize_dataframe(dataset)\n\u001b[0;32m--> 843\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts_preprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts_preprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# data preprocessing\u001b[39;00m\n\u001b[1;32m    851\u001b[0m ts_preprocessor\u001b[38;5;241m.\u001b[39mtrain(train_data)\n",
      "File \u001b[0;32m~/.conda/envs/csci8523/lib/python3.10/site-packages/tsfm_public/toolkit/time_series_preprocessor.py:778\u001b[0m, in \u001b[0;36mprepare_data_splits\u001b[0;34m(data, id_columns, context_length, split_config)\u001b[0m\n\u001b[1;32m    776\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m split_function[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m](data, id_columns\u001b[38;5;241m=\u001b[39mid_columns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    777\u001b[0m     valid_data \u001b[38;5;241m=\u001b[39m split_function[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m](data, id_columns\u001b[38;5;241m=\u001b[39mid_columns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 778\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m \u001b[43msplit_function\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m split_function(data, id_columns\u001b[38;5;241m=\u001b[39mid_columns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_params)\n",
      "File \u001b[0;32m~/.conda/envs/csci8523/lib/python3.10/site-packages/tsfm_public/toolkit/util.py:107\u001b[0m, in \u001b[0;36mselect_by_index\u001b[0;34m(df, id_columns, start_index, end_index)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of start_index or end_index must be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m id_columns:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_split_group_by_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    109\u001b[0m groups \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(_get_groupby_columns(id_columns))\n\u001b[1;32m    110\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/csci8523/lib/python3.10/site-packages/tsfm_public/toolkit/util.py:308\u001b[0m, in \u001b[0;36m_split_group_by_index\u001b[0;34m(group_df, name, start_index, end_index)\u001b[0m\n\u001b[1;32m    306\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelection would result in an empty time series, please check start_index and time series length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     msg \u001b[38;5;241m=\u001b[39m msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (id = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m msg\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Also check that end_index <= len(group_df)?\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m start_index:\n",
      "\u001b[0;31mValueError\u001b[0m: Selection would result in an empty time series, please check start_index and time series length"
     ]
    }
   ],
   "source": [
    "zeroshot_eval(dataset_name=TARGET_DATASET, context_length=CONTEXT_LENGTH, forecast_length=96, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_length=96,\n",
    "    batch_size=64,\n",
    "    fewshot_percent=5,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci8523",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
